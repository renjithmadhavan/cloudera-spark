# Spark - Day 2

spark-shell

sc

val myrdd = sc.textFile(
"file:/home/training/training_materials/data/frostroad.txt")

myrdd.count()

scala> myrdd.collect()

scala> val logfiles="/loudacre/weblogs/*"

scala> val logsRDD = sc.textFile(logfiles)

scala> val jpglogsRDD=
logsRDD.filter(line => line.contains(".jpg"))

scala> jpglogsRDD.take(10)

scala> sc.textFile(logfiles).
filter(line => line.contains(".jpg")).count()

scala> logsRDD.map(line => line.length).take(5)

scala> logsRDD.map(line => line.split(' ')).take(5)

scala> val ipsRDD =
logsRDD.map(line =>line.split(' ')(0))
scala> ipsRDD.take(5)

scala> ipsRDD.take(10).foreach(println)

scala> ipsRDD.saveAsTextFile("/loudacre/iplist")

val userRDD = logsRDD.map{t => 
val p = t.split(' ')
(p(0) + "/" + p(2))
}

userRDD.take(10).foreach(println)




// Upload data files to HDFS before running solution

// Example data:
//2014-03-15:10:10:33,Ronin S2,1a7eca8d-60c9-4d25-8609-d6cfd1ac80a1,0,24,82,72,enabled,enabled,enabled,41,62,36.49259162,-121.003629078
//2014-03-15:10:10:33/Titanic 2300/d86dbb9d-ff3c-40c6-8685-01f1fac45d9f/59/83/9/3/28/0/enabled/disabled/enabled/34.3456792864/-117.768326105

// Load the data file
val devstatus = sc.textFile("/loudacre/devicestatus.txt")

// Filter out lines with < 20 characters, use the 20th character as the delimiter, parse the line, and filter out bad lines
val cleanstatus = devstatus.
    filter(line => line.length>20).
    map(line => line.split(line.charAt(19))).
    filter(values => values.length == 14)
    
// Create a new RDD containing date, manufacturer, device ID, latitude and longitude
val devicedata = cleanstatus.
    map(values => (values(0), values(1).split(' ')(0), values(2), values(12), values(13)))

// Save to a CSV file as a comma-delimited string (trim parenthesis from tuple toString)
devicedata.
    map(values => values.toString).
    map(s => s.substring(1,s.length-1)).
    saveAsTextFile("/loudacre/devicestatus_etl")



########### Chapter 8 #####################

Exercise directory: $DEVSH/exercises/spark-pairs
Data files (HDFS): /loudacre/weblogs/*
/loudacre/accounts/*


// Step 1 - Create an RDD based on a subset of weblogs (those ending in digit 2)
val logs=sc.textFile("/loudacre/weblogs/*2.log")

// map each request (line) to a pair (userid, 1) then sum the hits
val userreqs = logs. 
   map(line => line.split(' ')).
   map(words => (words(2),1)).  
   reduceByKey((v1,v2) => v1 + v2)
   
// Step 2 - return a user count for each hit frequency
val freqcount = userreqs.map(pair => (pair._2,pair._1)).countByKey()

// Step 3 - Group IPs by user ID
val userips = logs. 
    map(line => line.split(' ')).
    map(words => (words(2),words(0))).
    groupByKey()
// print out the first 10 user ids, and their IP list
for (pair <- userips.take(10)) {
   println(pair._1 + ":")
   for (ip <- pair._2) println("\t"+ip)
}

// Step 4a - map account data to (userid,[values....])
val accountsdata="/loudacre/accounts/*"
val accounts = sc.textFile(accountsdata).
   map(line => line.split(',')).
   map(account => (account(0),account))
   
// Step 4b - Join account data with userreqs then merge hit count into valuelist   
val accounthits = accounts.join(userreqs)

// Step 4c - Display userid, hit count, first name, last name for the first few elements
for (pair <- accounthits.take(10)) {
   printf("%s %s %s %s\n",pair._1,pair._2._2, pair._2._1(3),pair._2._1(4))
}

#### bonus exercise ######

val accountsdata="/loudacre/accounts/*"
   
// Bonus 1: key accounts by postal/zip code
val accountsByPCode = sc.textFile(accountsdata).map(_.split(',')).keyBy(_(8))
 
// Bonus 2: map account data to lastname,firstname  
val namesByPCode = accountsByPCode.mapValues(values => values(4) + ',' + values(3)).groupByKey()

// Bonus 3: print the first 5 zip codes and list the names 
for (pair <- namesByPCode.sortByKey().take(5)) {
   println("---" + pair._1)
   pair._2.foreach(println)
}


############## DAY 3 #######################

Exercise directory: $DEVSH/exercises/spark-application
Data files (HDFS): /loudacre/weblogs
Scala project:
$DEVSH/exercises/spark-application/countjpgs_project
Scala classes: stubs.CountJPGs
solution.CountJPGs
Python stub: CountJPGs.py
Python solution:
$DEVSH/exercises/spark-application/pythonsolution/
CountJPGs.py


cd $DEVSH/exercises/spark-application/countjpgs_project
spark-submit --class stubs.CountJPGs \
--master yarn-client \
--name 'Count JPGs' \
target/countjpgs-1.0.jar /loudacre/weblogs/*


spark-submit --properties-file myspark.conf \
--class stubs.CountJPGs \
target/countjpgs-1.0.jar /loudacre/weblogs/*

sudo cp /usr/lib/spark/conf/log4j.properties.template /usr/lib/spark/conf/log4j.properties
